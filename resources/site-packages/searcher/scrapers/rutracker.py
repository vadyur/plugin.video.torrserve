#!/usr/bin/python
# -*- coding: utf-8 -*-

#import re
#import sys
import os
#import Cookie
#import string, xbmc, xbmcgui, xbmcplugin, xbmcaddon
#-------------------------------
import urllib, time, random
#from time import gmtime, strftime 
#from urlparse import urlparse 
#import json
import socket
socket.setdefaulttimeout(30)

import sys
if sys.version_info >= (3, 0):
    from urllib import request as urllib_request
    from urllib.parse import urlencode
else:
    import urllib2 as urllib_request
    from urllib import urlencode

from . import xt

#-----------------------------------------
#-----------------------------------------
siteUrl = 'nnmclub.to'
httpSiteUrl = 'http://' + siteUrl


# - ====================================== antizapret ====================================================
#import time, cookielib
#sid_file = os.path.join(xbmc.translatePath('special://temp/'), 'vpn.sid')
#cj = cookielib.FileCookieJar(sid_file) 
#hr  = urllib_request.HTTPCookieProcessor(cj) 
#Lthread=[]

def mfindal(http, ss, es):
    L=[]
    while http.find(es)>0:
        s=http.find(ss)
        e=http.find(es)
        i=http[s:e]
        L.append(i)
        http=http[e+2:]
    return L

def mfind(t,s,e):
    n = t.find(s)
    if n == -1: return ''
    r=t[n+len(s):]
    f = r.find(e)
    if f == -1: return ''
    r2=r[:f]
    return r2

def rulower(str):
    str=str.strip()
    str=xt(str).lower()
    str=str.replace('Й','й')
    str=str.replace('Ц','ц')
    str=str.replace('У','у')
    str=str.replace('К','к')
    str=str.replace('Е','е')
    str=str.replace('Н','н')
    str=str.replace('Г','г')
    str=str.replace('Ш','ш')
    str=str.replace('Щ','щ')
    str=str.replace('З','з')
    str=str.replace('Х','х')
    str=str.replace('Ъ','ъ')
    str=str.replace('Ф','ф')
    str=str.replace('Ы','ы')
    str=str.replace('В','в')
    str=str.replace('А','а')
    str=str.replace('П','п')
    str=str.replace('Р','р')
    str=str.replace('О','о')
    str=str.replace('Л','л')
    str=str.replace('Д','д')
    str=str.replace('Ж','ж')
    str=str.replace('Э','э')
    str=str.replace('Я','я')
    str=str.replace('Ч','ч')
    str=str.replace('С','с')
    str=str.replace('М','м')
    str=str.replace('И','и')
    str=str.replace('Т','т')
    str=str.replace('Ь','ь')
    str=str.replace('Б','б')
    str=str.replace('Ю','ю')
    return str

def lower(s):
    try:s=s.decode('utf-8')
    except: pass
    try:s=s.decode('windows-1251')
    except: pass
    s=s.lower().encode('utf-8')
    return s


def GET(target, referer='', post=None):
    try:
        req = urllib_request.Request(url = target, data = post)
        req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)')
        req.add_header('accept-encoding', 'gzip')
        resp = urllib_request.urlopen(req)
        if resp.info().get('Content-Encoding') == 'gzip':
            from io import StringIO
            import gzip
            buf = StringIO(resp.read())
            f = gzip.GzipFile(fileobj=buf)
            http = f.read()
        else:
            http=resp.read()
        resp.close()
        return http
    except Exception as e:
        print (e)

def POST(target, post=None, referer=''):
    try:
        req = urllib_request.Request(url = target, data = post)
        req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET4.0C)')
        req.add_header('Referer', referer)
        #req.add_header('X-Requested-With', 'XMLHttpRequest')
        resp = urllib_request.urlopen(req)
        http = resp.read()
        resp.close()
        http = http.decode(resp.info().get('Content-Type').split('charset=')[1], 'replace')
        http = http.encode('utf8')
#		print resp.geturl()
        return http
    except Exception as e:
        return ('')

def get_list(hp):#, info):
    #title=info['title']
    #print '== get_list =='
    #print len(hp)
    body = mfind(hp, '<tbody>','</tbody>')
    L=mfindal(body, '<tr class="prow', '</tr>')
    L2=[]
    for i in L: #[:2]:
        for s in i.splitlines():
            if 'td title="" class=' in s:
                ttl = mfind(s, '<b>', '<')
                ttd = mfind(s, 'gensmall opened">', '</span')
            if '"nowrap" class="gensmall"><u>' in s:
                size=mfind(s, '</u>', '</td>').replace('&nbsp;', ' ').replace(',','.').strip()
            if 'title="Seeders"' in s:
                seeds = mfind(s, '<b>', '<')
            if 'download.php?id=' in s:
                url = mfind(s, 'href="','" rel')
#		print ttl
#		print ttd
#		print size
#		print seeds
#		print url
        curl=httpSiteUrl+'/forum/'+url
#		print curl
        if ttd: ttl = ttl+' '+ttd
        itm ={"sids":seeds,"size":size, "title":ttl,"url":curl}#xt()
#		if int(seeds) > 5:
        L2.append(itm)
    
    return L2



class Tracker:
    def __init__(self):
        pass

    def Search(self, info):
        #print '====== wriza.top ====='
        #tmc = time.strftime('%d%y%m')
        t = info['originaltitle']
        if isinstance(t, str): t = t.decode('utf8')

        """ #
            string_to_search_converted = t.replace(' ', '+').encode('cp1251')
            payload = {
                'prev_sd' : 0,
                'prev_a' : 0,
                'prev_my' : 0,
                'prev_n' : 0,
                'prev_shc' : 0, # показывать колонку Категория
                'prev_shf' : 1, # показывать колонку Форум
                'prev_sha' : 1, # показывать колонку Автор
                'prev_shs' : 0, # показывать колонку Скорость
                'prev_shr' : 0, # показывать колонку Рейтинг
                'prev_sht' : 0, # показывать колонку Спасибо
                'f[]' : -1, # категории, где искать. '-1' = 'все имеющиеся'
                'o' : 1, # 1 - зарегистрирован, 4 - скачан, 7 - размер
                's' : 2, # 1 - по возрастанию, 2 - по убыванию
                'tm' : -1, # 30 - посл месяц, 7 - за посл неделю, -1 - за все время
                'shf' : 1,
                'sha' : 1,
                'ta' : -1,
                'sns' : -1,
                'sds' : -1,
                'nm' : string_to_search_converted,
                'pn' : '',
                'submit' : 'Поиск'
            }

        url=httpSiteUrl+'/forum/tracker.php'
#		print payload
        post=urlencode(payload)
        """

        from searcher.drivers.rutracker import RuTracker
        r  = RuTracker()
        se = r.search(t)['data']
        id = ''
#		print se
        for i in se:
            id += str(i['id'])+','
        id = id.strip(', ')
        if id != '': gttd = eval(GET('http://api.rutracker.org/v1/get_tor_topic_data?by=topic_id&val='+urllib.quote_plus(str(id))))
#		print gttd
        Lout = []
        for i in se:
            d = gttd['result'][str(i['id'])]
            ttl = i['name'].encode('utf8')
            param = {
                'dn': ttl,
                'tr': ["http://bt.t-ru.org/ann","http://bt2.t-ru.org/ann","http://bt3.t-ru.org/ann","http://bt4.t-ru.org/ann","http://bt5.t-ru.org/ann","http://bt2.t-ru.org/ann?magnet","http://retracker.local/announce"],
            }
            magnet = 'magnet:?xt=urn:btih:'+d['info_hash'].lower()+'&'+urlencode(param, True)
            Lout.append({'sids':d['seeders'], 'size': int(d['size']), 'title': ttl, 'url':magnet})
        
        return Lout

    def Query(self, query):
        try:
            return self.Search({'originaltitle': query})
        except Exception as e:
            print(e)
            return []

if __name__ == '__main__':
    print (Tracker().Query('ария'))
